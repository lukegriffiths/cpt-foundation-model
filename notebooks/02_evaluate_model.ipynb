{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abce3fa",
   "metadata": {},
   "source": [
    "# Model Evaluation on the Test Set\n",
    "\n",
    "This notebook evaluates the performance of the trained CPT Foundation Model on the held-out test set. The evaluation process consists of the following steps:\n",
    "\n",
    "1.  **Load Configuration and Model**: Load the same configuration file used for training and reinstantiate the model architecture. Load the trained weights from the saved `.pth` file.\n",
    "2.  **Load Test Data**: Use the `CPTDataModule` to get the `DataLoader` for the test split. This ensures we use the exact same data preprocessing and splits.\n",
    "3.  **Perform Inference**:\n",
    "    *   Iterate through the test set.\n",
    "    *   For each CPT profile, apply the same masking strategy used during training.\n",
    "    *   Feed the corrupted (masked) data to the model to get the reconstructions.\n",
    "4.  **Calculate Loss**: Compute the Mean Squared Error (MSE) between the model's predictions and the true values **only for the masked tokens**. This tells us how well the model can \"fill in the blanks.\"\n",
    "5.  **Visualize Results**: Plot a few examples from the test set, showing:\n",
    "    *   The original, complete CPT data.\n",
    "    *   The corrupted data with masked portions that were fed to the model.\n",
    "    *   The model's reconstructed output.\n",
    "\n",
    "This provides both a quantitative (MSE) and qualitative (visualization) assessment of the model's pre-training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcdb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Make sure the script can find the src modules\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), 'src')))\n",
    "\n",
    "from data_utils import CPTDataModule\n",
    "from model import CPTFoundationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b647a7",
   "metadata": {},
   "source": [
    "### 1. Load Configuration and Set Up Device\n",
    "We'll load the `PG_dataset.yaml` to ensure all our parameters (model dimensions, paths, etc.) are consistent with the training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'configs/PG_dataset.yaml'\n",
    "\n",
    "# Load the YAML configuration file\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration file loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{CONFIG_PATH}'\")\n",
    "    config = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration file: {e}\")\n",
    "    config = None\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50108d91",
   "metadata": {},
   "source": [
    "### 2. Load the Trained Model\n",
    "Instantiate the model with the parameters from the config file and then load the saved weights from the training process. It's crucial to set the model to evaluation mode using `.eval()` to disable layers like Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config:\n",
    "    model_params = config['model_params']\n",
    "    paths = config['data_paths']\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = CPTFoundationModel(\n",
    "        num_features=model_params['num_features'],\n",
    "        model_dim=model_params['model_dim'],\n",
    "        num_heads=model_params['num_heads'],\n",
    "        num_layers=model_params['num_layers']\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the saved model checkpoint\n",
    "    model_path = paths['model_save_path']\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        print(f\"Model loaded successfully from '{model_path}'\")\n",
    "        print(f\"Trained for {checkpoint.get('epoch', 'N/A')} epochs with a final loss of {checkpoint.get('loss', 'N/A'):.6f}\")\n",
    "    else:\n",
    "        print(f\"Error: Model file not found at '{model_path}'\")\n",
    "        model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35955999",
   "metadata": {},
   "source": [
    "### 3. Load the Test Dataset\n",
    "We use our `CPTDataModule` to handle the data setup. It will automatically find the processed data and load the correct test set based on the `test_ids.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09972a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config:\n",
    "    print(\"Setting up data module for the test set...\")\n",
    "    data_module = CPTDataModule(config)\n",
    "    data_module.setup() # This will set up train, val, and test datasets\n",
    "    \n",
    "    # Get the DataLoader for the test set\n",
    "    test_loader = data_module.get_dataloader(stage='test', shuffle=False)\n",
    "    print(\"Test data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10080f0",
   "metadata": {},
   "source": [
    "### 4. Run Evaluation and Calculate Loss\n",
    "Now we'll loop through the test set. In `torch.no_grad()` mode, we perform the forward pass to get the model's reconstructions and calculate the MSE loss on the masked values. We'll also store some examples for visualization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config and model:\n",
    "    total_mse = 0\n",
    "    total_masked_tokens = 0\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    # Store results for visualization\n",
    "    visualization_results = []\n",
    "    \n",
    "    # Get mask ratio from config\n",
    "    mask_ratio = config.get('training_params', {}).get('mask_ratio', 0.15)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"Evaluating on Test Set\")\n",
    "        for i, (batch, attention_mask) in enumerate(pbar):\n",
    "            batch = batch.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            # --- Create a corrupted version of the input batch ---\n",
    "            corrupted_batch = batch.clone()\n",
    "            prob_mask = torch.rand(batch.shape[:2], device=device)\n",
    "            masking_condition = (prob_mask < mask_ratio) & (attention_mask == 1)\n",
    "            \n",
    "            num_masked = masking_condition.sum().item()\n",
    "            if num_masked == 0:\n",
    "                continue # Skip batches where no tokens are masked\n",
    "            \n",
    "            corrupted_batch[masking_condition] = 0.0\n",
    "\n",
    "            # --- Forward Pass ---\n",
    "            predictions = model(corrupted_batch, attention_mask)\n",
    "            \n",
    "            # --- Calculate Loss on Masked Tokens ---\n",
    "            loss = loss_fn(predictions[masking_condition], batch[masking_condition])\n",
    "            total_mse += loss.item()\n",
    "            total_masked_tokens += num_masked\n",
    "            \n",
    "            # --- Store a few examples for visualization ---\n",
    "            if i < 5: # Store first 5 batches for potential visualization\n",
    "                visualization_results.append({\n",
    "                    'original': batch.cpu().numpy(),\n",
    "                    'masked': corrupted_batch.cpu().numpy(),\n",
    "                    'predicted': predictions.cpu().numpy(),\n",
    "                    'mask': masking_condition.cpu().numpy(),\n",
    "                    'attention_mask': attention_mask.cpu().numpy()\n",
    "                })\n",
    "\n",
    "    # Calculate the final average MSE across all masked tokens\n",
    "    average_mse = total_mse / total_masked_tokens if total_masked_tokens > 0 else 0\n",
    "    print(f\"\\nEvaluation Complete.\")\n",
    "    print(f\"Average MSE on masked tokens in the test set: {average_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032fd387",
   "metadata": {},
   "source": [
    "### 5. Visualize Reconstruction Results\n",
    "A quantitative metric like MSE is useful, but a qualitative visualization can provide deeper insight into the model's behavior.\n",
    "\n",
    "The following function plots a single CPT profile, comparing the ground truth, the masked input, and the model's reconstruction. We will focus on the first two numerical features, which are typically `qc` and `fs`. The masked regions in the reconstruction plot are highlighted in red to show exactly where the model was tasked with predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(original, masked, predicted, actual_mask, attention, feature_names=['qc', 'fs']):\n",
    "    \"\"\"\n",
    "    Plots a comparison of original, masked, and predicted CPT profiles for key features.\n",
    "    \n",
    "    Args:\n",
    "        original (np.array): The original data (single CPT).\n",
    "        masked (np.array): The data after masking (single CPT).\n",
    "        predicted (np.array): The model's output reconstruction (single CPT).\n",
    "        actual_mask (np.array): Boolean mask showing which tokens were masked.\n",
    "        attention (np.array): Attention mask showing real vs. padding data.\n",
    "        feature_names (list): Names of the features to plot.\n",
    "    \"\"\"\n",
    "    # Find the actual length of the sequence before padding\n",
    "    seq_len = int(attention.sum())\n",
    "    \n",
    "    # Trim all data to the actual sequence length\n",
    "    original = original[:seq_len]\n",
    "    masked = masked[:seq_len]\n",
    "    predicted = predicted[:seq_len]\n",
    "    actual_mask = actual_mask[:seq_len]\n",
    "\n",
    "    num_features = len(feature_names)\n",
    "    fig = make_subplots(rows=1, cols=num_features, subplot_titles=[f'Feature: {name}' for name in feature_names])\n",
    "    \n",
    "    depth = np.arange(seq_len)\n",
    "\n",
    "    for i, name in enumerate(feature_names):\n",
    "        # Plot Original Data\n",
    "        fig.add_trace(go.Scatter(x=original[:, i], y=depth, mode='lines', name='Original', line=dict(color='blue')), row=1, col=i+1)\n",
    "        \n",
    "        # Plot Model's Reconstruction\n",
    "        fig.add_trace(go.Scatter(x=predicted[:, i], y=depth, mode='lines', name='Reconstructed', line=dict(color='green')), row=1, col=i+1)\n",
    "        \n",
    "        # Highlight the masked areas that the model had to predict\n",
    "        masked_indices = np.where(actual_mask[:, i])[0]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=predicted[masked_indices, i], \n",
    "            y=depth[masked_indices], \n",
    "            mode='markers', \n",
    "            name='Predicted Masked Points', \n",
    "            marker=dict(color='red', size=6, symbol='x')\n",
    "        ), row=1, col=i+1)\n",
    "\n",
    "    fig.update_yaxes(autorange=\"reversed\", title_text=\"Depth Index\")\n",
    "    fig.update_layout(\n",
    "        title_text='Model Reconstruction vs. Original Data',\n",
    "        height=600,\n",
    "        width=900\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Let's visualize the first CPT profile from the first stored batch\n",
    "if visualization_results:\n",
    "    first_batch = visualization_results[0]\n",
    "    # Get the first CPT from the batch\n",
    "    cpt_index = 0 \n",
    "    \n",
    "    original_cpt = first_batch['original'][cpt_index]\n",
    "    masked_cpt = first_batch['masked'][cpt_index]\n",
    "    predicted_cpt = first_batch['predicted'][cpt_index]\n",
    "    mask_cpt = first_batch['mask'][cpt_index]\n",
    "    attention_cpt = first_batch['attention_mask'][cpt_index]\n",
    "    \n",
    "    visualize_reconstruction(original_cpt, masked_cpt, predicted_cpt, mask_cpt, attention_cpt)\n",
    "else:\n",
    "    print(\"No results available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53739bd2",
   "metadata": {},
   "source": [
    "You can re-run the cell above and change `cpt_index` to see other examples from the first batch, or change `visualization_results[0]` to `visualization_results[1]` to inspect results from a different batch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
